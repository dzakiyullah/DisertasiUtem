{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARY\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, jaccard_score, roc_auc_score, multilabel_confusion_matrix, roc_curve\n",
    "from scipy.sparse import issparse\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, jaccard_score, roc_auc_score, roc_curve\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "df_brfss = pd.read_csv('/Users/macbook/Library/CloudStorage/GoogleDrive-nurrachmandzakiyullah@gmail.com/My Drive/23. Colab Notebooks/1. AI_Project/2. PhD_Dzaki_BRFSS/Fix_Model_HP/3. Fix Model/Final_dataset_Diabetes_Complication.csv')\n",
    "x = df_brfss.iloc[:, :26].values\n",
    "X = preprocessing.normalize(x)\n",
    "y = df_brfss.iloc[:, 26:].values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix graph of the data set\n",
    "f, ax = plt.subplots(figsize= [20,15])\n",
    "sns.heatmap(df_brfss.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"rainbow\" )\n",
    "ax.set_title(\"Correlation Matrix\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train=pd.DataFrame(X_train)\n",
    "y_train=pd.DataFrame(y_train)\n",
    "X_train=X_train.to_numpy()\n",
    "y_train=y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decisions Tree Model\n",
    "# PROBLEM TRANSFORMATION\n",
    "base_classifier = RandomForestClassifier()\n",
    "model = base_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameter values\n",
    "params = base_classifier.get_params()\n",
    "\n",
    "# Print the parameter values\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with time\n",
    "start = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "training_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross-validation\n",
    "start = time.time()\n",
    "predicted_labels = cross_val_predict(model, X, y, cv=10)\n",
    "cross_val_time = time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with time\n",
    "start = time.time()\n",
    "predictions = model.predict(X_test)\n",
    "testing_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to dense array\n",
    "predictions_dense = predictions.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Matrix Evaluation Label Based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilabel confusion matrix\n",
    "cm = multilabel_confusion_matrix(y_test, predictions_dense)\n",
    "\n",
    "# Determine the grid dimensions for subplots\n",
    "num_labels = cm.shape[0]\n",
    "cols = min(num_labels, 2)\n",
    "rows = (num_labels + cols - 1) // cols\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 8))\n",
    "for i, (matrix, ax) in enumerate(zip(cm, axes.ravel())):\n",
    "    sns.heatmap(matrix, annot=True, fmt=\"d\", cbar=False, cmap='Blues', ax=ax)\n",
    "    ax.set_title(f\"Confusion Matrix - Label {i}\")\n",
    "    ax.set_xlabel(\"Predicted Labels\")\n",
    "    ax.set_ylabel(\"True Labels\")\n",
    "\n",
    "# Remove unused subplots if necessary\n",
    "if num_labels < rows * cols:\n",
    "    for j in range(num_labels, rows * cols):\n",
    "        fig.delaxes(axes.ravel()[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the roc_auc_per_label dictionary\n",
    "roc_auc_per_label = {}\n",
    "\n",
    "# Plot ROC curve for each label\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(y.shape[1]):\n",
    "    y_test_label = np.squeeze(y_test[:, i].toarray()) if issparse(y_test[:, i]) else np.squeeze(y_test[:, i])\n",
    "    y_pred_label = np.squeeze(predictions_dense[:, i].toarray()) if issparse(predictions_dense[:, i]) else np.squeeze(predictions_dense[:, i])\n",
    "    fpr, tpr, _ = roc_curve(y_test_label, y_pred_label)\n",
    "    roc_auc_per_label[i] = roc_auc_score(y_test_label, y_pred_label)\n",
    "    plt.plot(fpr, tpr, label='Label %d (AUC = %0.2f)' % (i, roc_auc_per_label[i]))\n",
    "\n",
    "# Plot aggregate AUC ROC\n",
    "fpr_aggregate, tpr_aggregate, _ = roc_curve(y_test.ravel(), predictions_dense.ravel())\n",
    "roc_auc_aggregate = roc_auc_score(y_test.ravel(), predictions_dense.ravel())\n",
    "plt.plot(fpr_aggregate, tpr_aggregate, label='Aggregate ROC Curve (AUC = %0.2f)' % roc_auc_aggregate)\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Set plot limits and labels\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the roc_auc_per_label dictionary\n",
    "roc_auc_per_label = {}\n",
    "\n",
    "# Plot ROC curve for each label\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(y.shape[1]):\n",
    "    y_test_label = np.array(y_test[:, i].toarray().ravel()) if issparse(y_test[:, i]) else np.array(y_test[:, i])\n",
    "    y_pred_label = np.array(predictions_dense[:, i].toarray().ravel()) if issparse(predictions_dense[:, i]) else np.array(predictions_dense[:, i])\n",
    "    fpr, tpr, _ = roc_curve(y_test_label, y_pred_label)\n",
    "    roc_auc_per_label[i] = roc_auc_score(y_test_label, y_pred_label)\n",
    "    plt.plot(fpr, tpr, label='Label %d (AUC = %0.2f)' % (i, roc_auc_per_label[i]))\n",
    "\n",
    "# Plot aggregate AUC ROC\n",
    "fpr_aggregate, tpr_aggregate, _ = roc_curve(y_test.ravel(), predictions_dense.ravel())\n",
    "roc_auc_aggregate = roc_auc_score(y_test.ravel(), predictions_dense.ravel())\n",
    "plt.plot(fpr_aggregate, tpr_aggregate, label='Aggregate ROC Curve (AUC = %0.2f)' % roc_auc_aggregate)\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Set plot limits and labels\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Matrix Evaluation Example Based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(y_test, predictions_dense)\n",
    "precision = precision_score(y_test, predictions_dense, average='micro')\n",
    "recall = recall_score(y_test, predictions_dense, average='micro')\n",
    "f1 = f1_score(y_test, predictions_dense, average='micro')\n",
    "hamming_loss_score = hamming_loss(y_test, predictions_dense)\n",
    "jaccard = jaccard_score(y_test, predictions_dense, average='micro')\n",
    "auc_roc = roc_auc_score(y_test, predictions_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print('Training Time:', round(training_time, 2), 'seconds')\n",
    "print('Cross-validated Testing Time:', round(cross_val_time, 2), 'seconds')\n",
    "print('Testing Time on Specific Test Set:', round(testing_time, 2), 'seconds')\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)\n",
    "print('Hamming Loss:', hamming_loss_score)\n",
    "print('Jaccard Score:', jaccard)\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "print('Exact Match Ratio: {0}'.format(sklearn.metrics.accuracy_score(y_test, predictions_dense, normalize=True, sample_weight=None)))\n",
    "print('Hamming loss: {0}'.format(sklearn.metrics.hamming_loss(y_test, predictions_dense))) \n",
    "print('Recall: {0}'.format(sklearn.metrics.precision_score(y_true=y_test, y_pred=predictions_dense, average='samples'))) \n",
    "print('Precision: {0}'.format(sklearn.metrics.recall_score(y_true=y_test, y_pred=predictions_dense, average='samples')))\n",
    "print('F1 Measure: {0}'.format(sklearn.metrics.f1_score(y_true=y_test, y_pred=predictions_dense, average='samples'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subet Accuracy atau Exact Match Ratio\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HITUNG HAMMING LOSS\n",
    "from sklearn.metrics import hamming_loss\n",
    "hamming_loss(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "print('jaccard_score_Samples: {0}'.format(jaccard_score(y_test, predictions, average='samples')))\n",
    "print('jaccard_score_Micro: {0}'.format(jaccard_score(y_test, predictions, average=\"micro\")))\n",
    "print('jaccard_score_Macro: {0}'.format(jaccard_score(y_test, predictions, average=\"macro\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "print('Zero_one_loss: {0}'.format(zero_one_loss(y_test, predictions)))\n",
    "print('Zero_one_loss_Samples: {0}'.format(zero_one_loss(y_test, predictions, normalize=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('f1_score_micro: {0}'.format(f1_score(y_test, predictions, average='micro')))\n",
    "print('f1_score_macro: {0}'.format(f1_score(y_test, predictions, average='macro')))\n",
    "print('f1_score_weighted: {0}'.format(f1_score(y_test, predictions, average='weighted')))\n",
    "print('f1_score_none: {0}'.format(f1_score(y_test, predictions, average=None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "print('fbeta_macro: {0}'.format(fbeta_score(y_test, predictions, average='macro', beta=0.5)))\n",
    "print('fbeta_micro: {0}'.format(fbeta_score(y_test, predictions, average='micro', beta=0.5)))\n",
    "print('fbeta_weighted: {0}'.format(fbeta_score(y_test, predictions, average='weighted', beta=0.5)))\n",
    "print('fbeta_none: {0}'.format(fbeta_score(y_test, predictions, average=None, beta=0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print('recision_recall_fscore_support_macro: {0}'.format(precision_recall_fscore_support(y_test, predictions, average='macro')))\n",
    "print('recision_recall_fscore_support_micro: {0}'.format(precision_recall_fscore_support(y_test, predictions, average='micro')))\n",
    "print('recision_recall_fscore_support_weighted: {0}'.format(precision_recall_fscore_support(y_test, predictions, average='weighted')))\n",
    "print('recision_recall_fscore_support_none: {0}'.format(precision_recall_fscore_support(y_test, predictions, average=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "print('precision_score_macro: {0}'.format(precision_score(y_test, predictions, average='macro')))\n",
    "print('precision_score_micro: {0}'.format(precision_score(y_test, predictions, average='micro')))\n",
    "print('precision_score_weighted: {0}'.format(precision_score(y_test, predictions, average='weighted')))\n",
    "print('precision_score_none: {0}'.format(precision_score(y_test, predictions, average=None)))\n",
    "print('precision_score_zero_divisio: {0}'.format(precision_score(y_test, predictions, average=None, zero_division=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "print('recall_score_macro: {0}'.format(recall_score(y_test, predictions, average='macro')))\n",
    "print('recall_score_micro: {0}'.format(recall_score(y_test, predictions, average='micro')))\n",
    "print('recall_score_weighted: {0}'.format(recall_score(y_test, predictions, average='weighted')))\n",
    "print('recall_score_none: {0}'.format(recall_score(y_test, predictions, average=None)))\n",
    "print('recall_score_zero_divisio: {0}'.format(recall_score(y_test, predictions, average=None, zero_division=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATION REPORT : precision, recacll f1-score support every label\n",
    "from sklearn.metrics import classification_report \n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual \\n\", y_test)\n",
    "print(\"\\nPredicted \\n\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
